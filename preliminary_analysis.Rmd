---
title: "Seattle Incarceration and Rentals with Glove Word Embeddings (sirGloVE)"
output: html_notebook
---

```{r load dependencies}
library(tidyverse)
library(stm)
library(sf)
library(tidycensus)
source('bin/helpers.R') # helper functions for writing text files
```

```{r}
data <- read_csv('data/cl_dropped_3_20.csv') # load cl data merged with topic proportions
tracts <- get_census_data()
```

```{r}
bbox <- st_bbox(tracts)
data <- data %>% 
  # convert the points to same CRS
  st_as_sf(coords = c("x", "y"),
           crs = st_crs(tracts))
# label tracts
data$tract <- tracts$GEO_ID_TRT[as.numeric(st_within(data, tracts))] 
```


```{r topic proportion over time}
# so I want to visualize topic prevalence over time....
# I should make time the x axis and then map topic prevalence as the y... take the average for each date and plot it as a dot?

merged_fit %>% select(Topic1:Topic40, listingDate) %>% group_by(listingDate) %>% summarise_all(mean) %>%
  gather("topic","topicPrevalence", Topic1:Topic40) %>% 
  #filter(topic %in% c("Topic10", 'Topic12', 'Topic14', 'Topic17', 'Topic19', 'Topic21', 'Topic26', 'Topic27', 'Topic30','Topic33','Topic34','Topic37','Topic4','Topic8','Topic9')) %>%
  #filter(topic == 'Topic27') %>%
  ggplot(aes(listingDate, topicPrevalence, color = topic))+
    #geom_point()+
    geom_smooth()+
    facet_wrap(~topic)

```
10, 12, 14, 17, 19, 21, 26, 27, 30,33,34,37,4,8,9

Topic 27 seems to follow the pattern of the approval and putting into effect of the law and includes the no RTSR language

```{r keyword proportion overtime}
# make a regex pattern that matches any of the stems starting with a space
keywords <- '\\scrim\\w*|\\srecord\\w*|\\sfelon\\w*|\\sviolen\\w*|\\sdrug\\w*|\\svandal\\w*|\\ssex\\w*|\\sassault\\w*|\\sconvict\\w*|\\sprior\\w*'
# a list of the same stems to check each keyword individually
stem_list <- c('crim', 'record', 'felon', 'violen', 'drug', 'vandal', 'sex', 'assault', 'convict', 'prior', 'evict', 'safe','secur')
# make a limited df that only has relavent columns
term_list <- read_lines('resources/termlist.txt')
texts_only <- merged_fit %>% select(listingDate, listingTitle, listingText, seattle) %>% mutate(fullText = paste(listingTitle, listingText))
# mark texts that have any keyword
texts_only <- texts_only %>% mutate(keyword_prevalence = str_detect(string = fullText, pattern = regex(keywords, ignore_case = TRUE)))

# make a column for each stem and mark if that stem is present
for(keyword in term_list){
  # format the stem as a regex pattern
  pattern = paste0('\\b',keyword,'\\b')
  # marke each text for presence of that stem
  texts_only[keyword] = str_detect(string = texts_only$fullText, pattern = regex(pattern, ignore_case = TRUE))
}

for(keyword in stem_list){
  # format the stem as a regex pattern
  pattern = paste0('\\b',keyword,'\\w*')
  # marke each text for presence of that stem
  texts_only[keyword] = str_detect(string = texts_only$fullText, pattern = regex(pattern, ignore_case = TRUE))
}


mtrix <- cor(texts_only%>%select(-starts_with('listing'),-fullText, - seattle))
mtrix>.4
summary(texts_only)
# take our df
texts_only %>%
  filter(seattle==0) %>% # filter on location
  select(-contains('text'), -listingTitle) %>% #get rid of text columns
  mutate(`Record/Convict/Sex/Drug` = record+convict+sex+drug, `Felon/Violen` = felon+violen) %>% #group some cols
  group_by(listingDate) %>% summarise_all(mean) %>% #group and summarise
  gather("keyword", "prevalence", - listingDate, - seattle) %>% # gather the keywords
  filter(keyword %in% c("crim", "record", "convict", "sex","drug")) %>%
  #filter(keyword %in% term_list) %>%
  #filter(keyword == 'crim') %>%
  ggplot(aes(listingDate, prevalence, color=keyword))+
    #facet_wrap(~keyword, nrow = 1)+
    #geom_point()+
    #geom_smooth(method='glm')+
    geom_smooth(method = 'loess', se = TRUE)+
    #theme(legend.position = "none")+
    theme_minimal()+
    geom_vline(aes(xintercept= as.Date('2017-07-01')), color = 'red')+
    geom_text(aes(x = as.Date('2017-07-01'), y = .02, label = "FiT"), nudge_x = 18)+
    geom_vline(aes(xintercept= as.Date('2018-02-19')), color = 'red')+
    geom_text(aes(x = as.Date('2018-02-19'), y = .03, label = "CRO"), nudge_x = 20)+
    labs(title = "Keyword Prevalence Over Time Outside", x = "Listing Date", y = "Proportion of texts including keyword")



```

peak in summer 17
drug
record
convict

peak in winter 18
felon
violent
sex

```{r export key texts}
locations <- c("outside_", "seattle_") # list of places
file_names <- c("before_7_1", "7_1_to_2_19", "2_19_to_3_29", "after_3_29") # of file names
dates <- c(as.Date('2016-01-01'), as.Date('2017-07-01'), as.Date('2018-02-19'), as.Date('2018-10-10')) # of datas
for(j in 0:1){
  for(i in 1:length(file_names)){ # loop through locations and dates
    frame <- merged_fit %>% 
      filter(listingDate>dates[i],listingDate<dates[i+1],seattle == j, Topic19>.2) %>% # grab the right texts
      mutate(temp_full = str_c(listingTitle,listingText), topic = 'Topic19')  #combine titles and text, append topic number
    docs = ifelse(length(frame$docnum)>20, 20, length(frame$docnum)) #sample up to 20 doduments
    walk(sample_n(frame, docs)$py_index, text_output, df=frame, filepath = paste0('output/text/extras/'), type = 'document', extra = TRUE) # use helper function ot make nicely formatted texts
  }
}
```

```{r save cleaned text as a file with one line per sentence}
library(tokenizers)
locations <- c("outside_", "seattle_") # list of places
file_names <- c("before_7_1.txt", "7_1_to_2_19.txt", "2_19_to_3_29.txt", "after_3_29.txt") # of file names
dates <- c(as.Date('2016-01-01'), as.Date('2017-07-01'), as.Date('2018-02-19'), as.Date('2018-03-29'), as.Date('2018-11-29')) # of dates
for(j in 0:1){
  for(i in 1:length(file_names)){ # loop through locations and dates
    frame <- merged_fit %>% 
      filter(listingDate>dates[i],listingDate<dates[i+1],seattle == j) %>% # grab the right texts
      mutate(temp_full = str_c(listingTitle,listingText)) # combine titles and text
    list <- frame$temp_full # make a list of the right texts
    write_tokenized_sentences(list, paste0(locations[j+1], file_names[i])) # helper function to save nicely
    print(paste("Saved", length(list), "texts for ", locations[j+1], file_names[i])) # log number of files
  }
}
write_tokenized_sentences(str_c(merged_fit$listingTitle, merged_fit$listingText), 'sent_tokenized.txt')
```

